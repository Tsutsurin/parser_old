from selenium import webdriver
from selenium.webdriver.edge.service import Service
from collections import Counter
from bs4 import BeautifulSoup
from datetime import datetime
import pandas as pd
import time
import random
import requests
import re

number = []
source = []
status = []
all_problem_data = []
all_problem_cve = []
all_problem_cvss = []
all_problem_product = []
all_problem_url = []
all_problem_cve_edited = []
all_problem_cvss_edited = []

num_pages = int(input("number "))
proxy = input("proxy ")

service = Service(r'msedgedriver.exe')
options = webdriver.EdgeOptions()

options.add_argument("--disable-blink-features=AutomationControlled")
options.add_argument("--disable-infobars")
options.add_argument("--start-maximized")
options.add_argument("--ignore-certificate-errors")
# options.add_argument("--headless")

driver = webdriver.Edge(options=options, service=service)

for page in range(1, num_pages + 1):
    url = "https://bdu.fstec.ru/vul?sort=datv&page=" + str(page)

    driver.get(url)

    html = driver.page_source

    time.sleep(random.uniform(2, 3))
    soup = BeautifulSoup(html, "lxml")

    tds = soup.find_all("td", class_="col-lg-3 col-xs-3")

    for td in tds:
        links = td.find_all("a", class_="confirm-vul")

        for link in links:
            href = link["href"]

            vul_link = "https://bdu.fstec.ru" + href
            all_problem_url.append(vul_link)

for vul_link in all_problem_url:
    driver.get(vul_link)

    html = driver.page_source

    time.sleep(random.uniform(2, 3))

    soup = BeautifulSoup(html, "lxml")
    tds = soup.find_all("td")

    problem_product = tds[3].text.strip() + " " + tds[5].text.strip()

    cvss = tds[23].text.strip()
    pattern = r"\d+(?:,\d+)?"
    number = re.findall(pattern, cvss)
    size = len(number) - 1
    if any(map(str.isnumeric, cvss)):
        problem_cvss = number[size].replace(',', '.')
        all_problem_cvss.append(problem_cvss)
    else:
        print('Ошибка: список cvss не содержит цифр')

    cve = tds[39].text.strip()
    pattern = r"CVE: "
    problem_cve = re.sub(pattern, "", cve)
    all_problem_cve.append(problem_cve)

    source.append("ФСТЭК")
    all_problem_product.append(problem_product)
    all_problem_data.append(tds[19].text.strip())

driver.close()

# NKTSKI
proxy = {
    "http": proxy,
    "https": proxy
}

for page in range(1, num_pages + 1):
    url = "https://safe-surf.ru/specialists/bulletins-nkcki/?PAGEN_1=" + str(page)
    link = requests.get(url, proxies=proxy)
    soup = BeautifulSoup(link.text, "lxml")
    time.sleep(5)

    problem_links = [elem.get("href") for elem in soup.find_all("a", title="Подробнее")]
    problem_url = ["https://safe-surf.ru" + link for link in problem_links]

    problem_data = [elem.get_text().strip().replace("Дата бюллетеня", "") for elem in
                    soup.find_all(class_="cell-bulletin-nkcki cell-1")]
    problem_data = list(filter(bool, problem_data))
    problem_data = [elem.lstrip() for elem in problem_data]

    problem_cve = [elem.get_text().strip().replace("Идентификатор уязвимости", "") for elem in
                   soup.find_all(class_="cell-bulletin-nkcki cell-2")]
    problem_cve = list(filter(bool, problem_cve))
    problem_cve = [elem.lstrip() for elem in problem_cve]
    problem_cve = [elem.strip().replace("MITRE:", "") for elem in problem_cve]

    problem_product = [elem.get_text().strip().replace("Уязвимый продукт", "") for elem in
                       soup.find_all(class_="cell-bulletin-nkcki cell-3")]
    problem_product = list(filter(bool, problem_product))
    problem_product = [elem.lstrip() for elem in problem_product]
    problem_product = [elem.replace("\n", "") for elem in problem_product]
    problem_product = [re.sub(
        "                                                                                                                                 ",
        "     ", elem) for elem in problem_product]

    problem_cvss = [elem.get_text().strip().replace("Уровень опасности", "") for elem in
                    soup.find_all(class_="cell-bulletin-nkcki cell-4")]
    problem_cvss = list(filter(bool, problem_cvss))
    problem_cvss = [elem.lstrip() for elem in problem_cvss]
    problem_cvss = [elem.replace("\n", "") for elem in problem_cvss]
    problem_cvss = [re.sub("\\s+", " ", elem) for elem in problem_cvss]

    all_problem_cve.extend(problem_cve)
    all_problem_cvss.extend(problem_cvss)

    all_problem_data.extend(problem_data)
    all_problem_product.extend(problem_product)
    all_problem_url.extend(problem_url)

    for _ in range(len(problem_data)):
        source.append("НКЦКИ")

for cve in all_problem_cve:
    cve = cve.replace("\u00a0", " ")
    matches = re.findall("CVE-\d{4}-\d{4,}", cve)
    if matches:
        match = matches[0]
        all_problem_cve_edited.append(match)
    else:
        all_problem_cve_edited.append("Zero-day")

for j in range(len(all_problem_cvss)):
    pattern = r"\d+(?:.\d+)?"
    number = re.findall(pattern, all_problem_cvss[j])
    if len(number) == 1:
        if float(number[0]) < 4:
            all_problem_cvss[j] = f"{number[0]} Low"
        elif float(number[0]) < 7:
            all_problem_cvss[j] = f"{number[0]} Medium"
        elif float(number[0]) < 9:
            all_problem_cvss[j] = f"{number[0]} High"
        else:
            all_problem_cvss[j] = f"{number[0]} Critical"
    else:
        all_problem_cvss[j] = "-"
    all_problem_cvss_edited.append(all_problem_cvss[j])

counter = Counter(all_problem_cve_edited)
dates = [datetime.strptime(date, "%d.%m.%Y") for date in all_problem_data]
for index, value in enumerate(all_problem_cve_edited):
    number.append(index + 1)
    if counter[value] > 1:
        indices = [i for i, x in enumerate(all_problem_cve_edited) if x == value]
        dates_list = [dates[i] for i in indices]
        min_date = min(dates_list)
        if dates[index] == min_date:
            status.append("Новый")
        else:
            if dates[index] > min_date:
                status.append("Повтор")
    else:
        status.append("Новый")

df = pd.DataFrame({"№": number,
                   "Источник": source,
                   "Дата публикации": all_problem_data,
                   "Статус": status,
                   "CVE": all_problem_cve_edited,
                   "CVSS": all_problem_cvss_edited,
                   "Продукт": all_problem_product,
                   "Ссылка": all_problem_url})

with pd.ExcelWriter('problem_table.xlsx') as writer:
    df.to_excel(writer, sheet_name='Проблемы', index=False)
